# 重力四子棋

[Eren Zhao](https://zhaochenyang20.github.io/)

赵晨阳 计 06 2020012363

# 强化学习

## 概念

 强化学习是机器学习中的一个领域，强调如何基于环境而行动，以取得最大化的预期利益。其灵感来源于心理学中的行为主义理论，即有机体如何在环境给予的奖励或惩罚的刺激下，逐步形成对刺激的预期，产生能获得最大利益的习惯性行为。这个方法具有普适性，因此在其他许多领域都有研究，例如博弈论、控制论、运筹学、信息论、仿真优化、多主体系统学习、群体智能、统计学以及遗传算法。在运筹学和控制理论研究的语境下，强化学习被称作“近似动态规划”（approximate dynamic programming，ADP）。在最优控制理论中也有研究这个问题，虽然大部分的研究是关于最优解的存在和特性，并非是学习或者近似方面。在经济学和博弈论中，强化学习被用来解释在有限理性的条件下如何出现平衡。

在机器学习问题中，环境通常被规范为**马尔可夫决策过程（MDP）**，许多强化学习算法在这种情况下使用动态规划技巧。 强化学习和标准的监督式学习之间的区别在于，它**并不需要出现正确的输入/输出对，也不需要精确校正次优化的行为**。

强化学习更加专注于在线规划，需要在探索（在未知的领域）和遵从（现有知识）之间找到平衡。强化学习中的“探索-遵从”的交换，在多臂老虎机（英语：multi-armed bandit）问题和有限MDP中研究得最多。

## 强化学习的基本组件

- 环境/状态（标准的为静态stationary，对应的non-stationary）
- agent（与环境交互的对象）
- 动作（action space，环境下可行的动作集合，离散/连续）
- 反馈（回报，reward，正是有了反馈，RL才能迭代，才会学习到策略链）

# 马尔可夫决策过程（MDP）

## 马尔可夫过程

 在概率论及统计学中，马尔可夫过程（Markov process）又叫马尔可夫链(Markov Chain)，是一个具备了马尔可夫性质的随机过程，因为俄国数学家安德雷·马尔可夫得名。马尔可夫过程是不具备记忆特质的（memorylessness）。换言之，马尔可夫过程的条件概率仅仅与系统的当前状态相关，而与它的过去历史或未来状态，都是独立、不相关的。马尔可夫过程可以用一个元组表示，其中 S 是有限数量的状态集，P 是状态转移概率矩阵。

## 马尔可夫奖励过程

 马尔可夫奖励过程（Markov Reward Process）在马尔可夫过程的基础上增加了奖励 R 和衰减系数 γ。R 是一个奖励函数。S 状态下的奖励是某一时刻 t 处在状态 s 下在下一个时刻 t+1 能获得的奖励期望（当进入某个状态会获得相应的奖励）。

## RL 与 MDP

 在强化学习中，马尔可夫决策过程是对完全可观测的环境进行描述的，也就是说观测到的状态内容完整地决定了决策的需要的特征。几乎所有的强化学习问题都可以转化为 MDP。

# 蒙特卡洛方法（MCM）

## 简介

蒙特卡罗方法（Monte Carlo Method），也称统计模拟方法，是1940年代中期由于科学技术的发展和电子计算机的发明，而提出的一种以概率统计理论为指导的数值计算方法。是指使用随机数（或更常见的伪随机数）来解决很多计算问题的方法。

 20世纪40年代，在冯·诺伊曼，斯塔尼斯拉夫·乌拉姆和尼古拉斯·梅特罗波利斯在洛斯阿拉莫斯国家实验室为核武器计划工作时，发明了蒙特卡罗方法。因为乌拉姆的叔叔经常在摩纳哥的蒙特卡洛赌场输钱得名，而蒙特卡罗方法正是以概率为基础的方法。

 通常蒙特卡罗方法可以粗略地分成两类：一类是所求解的问题本身具有内在的随机性，借助计算机的运算能力可以直接模拟这种随机的过程。例如在核物理研究中，分析中子在反应堆中的传输过程。中子与原子核作用受到量子力学规律的制约，人们只能知道它们相互作用发生的概率，却无法准确获得中子与原子核作用时的位置以及裂变产生的新中子的行进速率和方向。科学家依据其概率进行随机抽样得到裂变位置、速度和方向，这样模拟大量中子的行为后，经过统计就能获得中子传输的范围，作为反应堆设计的依据。

 另一种类型是所求解问题可以转化为某种随机分布的特征数，比如随机事件出现的概率，或者随机变量的期望值。通过随机抽样的方法，以随机事件出现的频率估计其概率，或者以抽样的数字特征估算随机变量的数字特征，并将其作为问题的解。这种方法多用于求解复杂的多维积分问题。

例如，下图阐释了如何利用蒙特卡洛方法来估算 $\pi$ 的值：

<img src="https://zhaochenyang20.github.io/gif/pi.gif" style="zoom:70%;" />

## 前景

就单纯的用蒙特卡洛方法来下棋（最早在1993年被提出，后在2001被再次提出），我们可以简单的用随机比赛的方式来评价某一步落子。从需要评价的那一步开始，双方随机落子，直到一局比赛结束。为了保证结果的准确性，这样的随机对局通常需要进行上万盘，记录下每一盘的结果，最后取这些结果的平均，就能得到某一步棋的评价。最后要做的就是取评价最高的一步落子作为接下来的落子。也就是说为了决定一步落子就需要程序自己进行上万局的随机对局，这对随机对局的速度也提出了一定的要求。和使用了大量围棋知识的传统方法相比，这种方法的好处显而易见，就是几乎不需要围棋的专业知识，只需通过大量的随机对局就能估计出一步棋的价值。再加上一些优化方法，基于纯蒙特卡洛方法的围棋程序已经能够匹敌最强的传统围棋程序。

 既然蒙特卡洛的路似乎充满着光明，我们就应该沿着这条路继续前行。MCTS也就是将以上想法融入到树搜索中，利用树结构来更加高效的进行节点值的更新和选择。

# 蒙特卡洛树搜索（MCTS）

## 简介

 蒙特卡洛树搜索（Monte Carlo tree search；MCTS）是一种用于某些决策过程的启发式搜索算法，最引人注目的是在游戏中的使用。一个主要例子是电脑围棋程序，它也用于其他棋盘游戏、即时电子游戏以及不确定性游戏。

## 搜索步骤

<img src="https://zhaochenyang20.github.io/pic/lecture/2022_spring/IAI/MCTS.jpg" alt="MCTS" style="zoom:50%;" />

- 选择(selection)：根据当前获得所有子步骤的统计结果，选择一个最优的子步骤。从根结点 R 开始，选择连续的子结点向下至叶子结点 L 。一般而言，让游戏树向最优的方向扩展，这是蒙特卡洛树搜索的精要所在。
- 扩展(expansion)：在当前获得的统计结果不足以计算出下一个步骤时，随机选择一个子步骤。除非任意一方的输赢使得游戏在 L 结束，否则创建一个或多个子结点并选取其中一个结点 C。
- 模拟(simulation)：模拟游戏，进入下一步。在从结点C开始，用随机策略进行游戏，又称为playout或者rollout。
- 反向传播(Back-Propagation)：根据游戏结束的结果，计算对应路径上统计记录的值。使用随机游戏的结果，更新从C到R的路径上的结点信息。
- 决策（decision）：当到了一定的迭代次数或者时间之后结束，选择根节点下最好的子节点作为本次决策的结果。

<img src="https://zhaochenyang20.github.io/pic/lecture/2022_spring/IAI/MCTS2.jpg" alt="MCTS" style="zoom:30%;" />

## 具体算法

在开始阶段，搜索树只有一个节点，也就是我们需要决策的局面。

搜索树中的每一个节点包含了三个基本信息：代表的局面，被访问的次数，累计评分。

### 选择

在选择阶段，需要从根节点，也就是要做决策的局面 R 出发向下选择出一个最急迫需要被拓展的节点 N，局面 R 是每一次迭代中第一个被检查的节点；

对于被检查的局面而言，他可能有三种可能：

1. 该节点所有可行动作（即所有子节点）都已经被拓展过
2. 该节点有可行动作（还有子节点）还未被拓展过
3. 这个节点游戏已经结束了(例如已经连成四子的四子棋局面)

对于这三种可能：

1. 如果所有可行动作都已经被拓展过，即所有子节点都有了战绩，那么我们将使用 UCB 公式计算该节点所有子节点的 UCB 值，并找到值最大的一个子节点继续向下迭代。
2. 如果被检查的节点 A 依然存在没有被拓展的子节点 B (也即还有战绩为 0/0 的节点)，那么我们认为 A 节点就是本次迭代的的目标节点，紧接着对 A 进行扩展。
3. 如果被检查到的节点是一个游戏已经结束的节点。那么从该节点直接记录战绩，并且反向传播。

### 扩展

在选择阶段结束时候，我们查找到了一个最迫切被拓展的节点 N，以及他一个尚未拓展的动作 A。在搜索树中创建一个新的节点 $N_A$ 作为N的一个新子节点，$N_A$ 的局面就是节点 N 在执行了动作 A 之后的局面。

### 模拟

为了让 $N_A$ 得到一个初始的评分,我们从 $N_A$ 开始，让游戏随机进行，直到得到一个游戏结局，这个结局将作为 $N_A$ 的初始战绩，采用 $\frac{胜场}{总次数}$来记录。

### 反向传播

 在 $N_A$ 的模拟结束之后，它的父节点 n 以及从根节点到 N 的路径上的所有节点都会根据本次模拟的结果来添加自己的累计评分，注意评分具有交替性。如果在选择阶段直接造成了游戏结局，则跳过模拟，根据该结局来更新评分。

### 决策

 每一次迭代都会拓展搜索树，随着迭代次数的增加，搜索树的规模也不断增加。当到了一定的迭代次数或者时间之后结束，选择根节点下最好的子节点作为本次决策的结果。

<img src="https://zhaochenyang20.github.io/pic/lecture/2022_spring/IAI/MCTS4.jpg" alt="MCTS" style="zoom:37%;" />

<img src="https://zhaochenyang20.github.io/pic/lecture/2022_spring/IAI/MCTS3.jpg" alt="MCTS" style="zoom:50%;" />

简化后的中文流程图如下：

<img src="https://zhaochenyang20.github.io/pic/lecture/2022_spring/IAI/MCTS5.png" alt="MCTS" style="zoom:37%;" />



# UCT

## UCB1

$$
UCB1 = V_i + c\sqrt{\frac{\ln{N}}{n_i}}
$$

- $V_i$ 表示当前节点 i 的战绩。
- $N$ 为当前节点的父节点被访问的次数。
- $n_i$ 为当前节点i被访问的次数。
- c 为探索参数，在实际中通常可凭经验选择。c 越大，就会越照顾访问次数相对较少的子节点。

## MCTS + UCB1

UCT 算法（Upper Confidence Bound Apply to Tree）即上限置信区间算法，是一种博弈树搜索算法，该算法将蒙特卡洛树搜索方法与UCB公式结合，在超大规模博弈树的搜索过程中相对于传统的搜索算法有着时间和空间方面的优势。

即：MCTS + UCB1 = UCT

算法中的 UCB 公式可替换为：UCB1-tuned 等

UCT 提供了比传统树搜索更好的方法。

### 优点

- Aheuristic 启发式

  MCTS 不要求任何关于给定的领域策略或者具体实践知识来做出合理的决策。这个算法可以在没有任何关于博弈游戏除基本规则外的知识的情况下进行有效工作；这意味着一个简单的MCTS 实现可以重用在很多的博弈游戏中，只需要进行微小的调整，所以这也使得 MCTS 是对于一般的博弈游戏的很好的方法。

- Asymmetric 非对称

  MCTS 执行一种非对称的树的适应搜索空间拓扑结构的增长。这个算法会更频繁地访问更加有趣的节点，并聚焦其搜索时间在更加相关的树的部分。这使得 MCTS 更加适合那些有着更大的分支因子的博弈游戏，比如说 19X19 的围棋。这么大的组合空间会给标准的基于深度或者宽度的搜索方法带来问题，所以MCTS 的适应性说明它（最终）可以找到那些更加优化的行动，并将搜索的工作聚焦在这些部分。

- 任何时间

  算法可以在任何时间终止，并返回当前最有的估计。当前构造出来的搜索树可以被丢弃或者供后续重用。（对比dfs暴力搜索）

- 简洁

  算法实现非常方便（ http://mcts.ai/code/python.html ）

### 缺点

UCT 有缺点很少，但这些缺点也可能是非常关键的影响因素。

- 行为能力

  UCT 算法，根据其基本形式，在某些甚至不是很大的博弈游戏中在可承受的时间内也不能够找到最好的行动方式。这基本上是由于组合步的空间的全部大小所致，关键节点并不能够访问足够多的次数来给出合理的估计。

- 速度

  UCT 搜索可能需要足够多的迭代才能收敛到一个很好的解上，这也是更加一般的难以优化的应用上的问题。例如，最佳的围棋程序可能需要百万次的交战和领域最佳和强化才能得到专家级的行动方案，而最有的GGP 实现对更加复杂的博弈游戏可能也就只要每秒钟数十次（领域无关的）交战。对可承受的行动时间，这样的 GGP 可能很少有时间访问到每个合理的行动，所以这样的情形也不大可能出现表现非常好的搜索。

# 具体实现

- 为了简洁优雅地实现我的 MCTS 策略，我使用 `enhencement.h` 来实现了 `Node` 类和 `UCT` 类
- `Node` 类接口如下：

```C++
class Node
  double profit;                  // 当前节点的胜率
  int visit;                        // 总访问次数
  int ban_x, ban_y;            // 被去除的点位
  int height;                      // 棋盘高度
  int width;                       // 棋盘宽度
  int expandableNodeNum;            // 可扩展节点数
  int position_x, position_y;  // 落子位置
  bool expanded;                 // 是否已经扩展
  bool chance;                  // 是否为己方棋子
  int** boardStatus;  // 当前局面状况
  int* topStatus;     // 当前每一列顶部状况
  Node* parent;  // 父节点
  Node** children;     // 子节点
	expand; // 扩展，用于 treePolicy 中调用
	Node // 构造函数
  ~Node // 析构函数
  int* expandableNodeID;  // 从当前节点开始可扩展节点的行号
 	int must(chance)	// 判定需要紧急处理的棋局情况，例如三子共线
  bestChild; // 在所有子节点中选择 UCB1 值最高的节点
```

- `UCT` 类接口如下：

```c++
class UCT
  width, height;  // 棋盘规格
  ban_x, ban_y;   // 被去除的点位
  expanded;      // 是否已经展开
  Node* root;		// 根节点
	UCT // 构造函数
  ~UCT // 析构函数
  middle_the_best // 用来实现下文所述的中庸之道
  search // 在时间限制内不断运行伪代码，之后给出根节点的最佳子节点
  treePolicy // 即伪代码中的 treePolicy，不再赘述
  defaultPolicy // 即伪代码中的 defaultPolicy，不再赘述
```

# 可行的优化方向

总地来说，本次实验采用的是MCTS算法。在实现我认为有如下细节可以优化： 

## 反向传播时的参数更新

与马老师课堂所讲述方法略有不同，在具体实现时，我令 Rollout 过程中，己方赢的回报为 1，对手赢的回报为 -1，平局回报为 0。 

该回报值反向地沿当轮迭代的路径从被 Rollout 节点向根节点传播。具体来说，如果一个节点是极大节点 （说明其父节点为极小节点），则该节点的值减去回报值；如果一个节点是极小节点，则该节点的值加上回 报值。

选择上述反向传播的更新方法时，UCB评估算式对于所有节点都是一样的，不必区分极大与极小节点。 （相反，若在反向传播过程中没有区分极大与极小节点，则UCB算式需要区分。）

## 三连攻防

参考 `judge.cpp` 中已经实现了的判定输赢所用的 `userWin` 与 `machineWin` 函数，二者判定了四点共线的情况。

联系到在下五子棋的过程中，每当出现三个点连成一片时，就需要执行攻防策略了，四子棋中理应如此。然而四子棋需考虑重力因素，因此具有独特的攻防特性。

具体而言，我在 `Node` 类中实现了 `must` 函数，用于判定是否有地方三子共线且是敌方是否能连上第四子。如果敌方能够立刻连上第四子，则我方直接跳过模拟过程，堵住敌方的第四子，此所谓三连攻防问题。

## 中庸之道

> 中庸之为德也，其至矣乎，民鲜久矣！——《论语》
>
> “本手、妙手、俗手”是围棋的三个术语。本手是指合平棋理的正规下法；妙手是指出人意料的 精妙下法；俗手是指貌似合理，而从全局看通常会受损的下法。对于初学者而言，应该从本手开始， 本手的功夫扎实了，棋力才会提高。一些初学者热衣于追求妙手，而忽视更为常用的本手。本手是基础，妙手是创造。一般来说，对本手理解深刻，才可能出现妙手；否则，难免下出俗手，水平也不易提升。——《2022 年全国卷新高考 I 卷语文作文试题》

在五子棋中，一般而言，向棋盘中间挺进的落子更容易成为妙手，而越往角落挺进的棋子，多为不得已为止，大抵为本手，更有很大可能沦为俗手。

在四子棋的对弈过程中，也有类似的规律，由于重力原因，四子棋的棋盘中央即为棋盘底部的中央，于是我考虑 `defaultPolicy` 随机走子的基础上，更改底部各子的权重，增大中部的权重，更容易将棋子下在中部，这样下出妙手的可能性更高，提高了有效模拟的比例。

具体而言，假设棋盘宽度为 $2k+1$，则底部棋盘的权重从左侧到右侧依次为 $1,2,\cdots k,k+1,k,\cdots,2,1 $，倘若棋盘宽度为 $2k$，则权重为 $1,2,\cdots,k,k,\cdots,2,1$，并按此权重选择棋子，具体的选择方法参考 `middle_the_best()` 函数。

## 信息储存

基本的实现过程中，每个棋子都储存了一次整个棋盘的信息，可以预料到会造成大量的内存浪费。可以在每轮迭代之前创建一个初始棋盘（自行实现的Board类，存储处于某种状态的棋盘），每轮迭代开始时复制一个初始棋盘。由于MCTS算法在搜索树中的路径是一直向下的，因此可以根据节点（Node类）存储的其相对于父节点多走的一步棋的信息，边向下搜索边更新棋盘。这样不用每个节点存储一个棋盘，占用大量空间。 

另外，考虑到对于搜索树上的节点，如果采用 C++ 中 new 的方式每次逐个创建，则会由于 new 操作的常数以及访问不连续的地址浪费较多时间。可以考虑利用内存池的方法来优化这一内存损耗。

## 最终策略选择

我在最终一步选择了 UCB1 值最大的节点，达到了 90% 以上的胜率。然而如果仅仅将这一步改为选择被访问次数最多的节点，在 Saiblo 网站上使用 50 个样例 AI 进行评测的正确率就会由 90% 以上下降到 50% 左右。

鉴于实现策略存在明显错误的 AI 在 Saiblo 网站上的正确率仍为 40%-50%（此现象为通过一些尝试发现），可知后一种选择方式是极其糟糕的。最终选择还是 UCB 评估值最大策略。

## 参数优化

本次实验主要能够调整的参数是 `timelimit` 与 UCB1 算法中的 `C` 值。

对于时限参数，这一参数显著收到了测评机稳定性的影响。某天晚上我测试时，将 `timelimit` 设置为 `2.7 * CLOCKS_PER_SEC`，拿到了 97% 的胜率，第二天上午再原样测试，胜率显著降低，直接到了 60%，而失败原因为大量的 TLE。

起初我试图联系平台助教，请求修复这个 bug，然而我事后想想，作为一个运维工作中，服务器波动实属正常。我应该尽力从算法和 C++ 语言特性去优化我的 AI，而不是限制运维人员，遂放弃了苛责服务器的想法。

接着是 `C` 值，受限于批量测试次数有限，我只能手动调整参数，在服务器性能较好的一天晚上，我以 `2.7 * CLOCKS_PER_SEC` 为时限，与实力较为强劲的 15 个机器人对弈，得到的结果如下：

| C 参数 | 胜率 |
| ------ | ---- |
| 0.7    | 70   |
| 0.6    | 76   |
| 0.575  | 80   |
| 0.55   | 83   |
| 0.525  | 73   |
| 0.5    | 66   |

对打的 AI 为：`<<Connect4_100>> <<Connect4_94>> <<Connect4_92>> <<Connect4_96>> <<Connect4_98>> <<Connect4_90>> <<Connect4_84>> <<Connect4_74>> <<Connect4_76>> <<Connect4_80>> <<Connect4_88>> <<Connect4_78>> <<Connect4_86>> <<Connect4_82>> <<Connect4_70>>`

c 为探索参数，在实际中通常可凭经验选择。c 越大，就会越照顾访问次数相对较少的子节点。一般而言，C 越大越激进，反之则保守。我选择了相对激进的 0.7 作为最终参数。

## 带策略出棋

无策略指在 Rollout 过程中的每一步，采用完全随机的方式走子。而这一步可以优化为带策略出棋：

1. 遍历所有可能的走子方式。若当前角色能在某处落子后获胜，则本次模拟立即以这种方式结束，并返回相应的结果。 （此即我所实现的 `must` 方法）

2. 若当前角色没有能一步获胜的方法，则再次遍历所有可能的走子方式。若当前角色的对手能在某处落子后获胜，则该模拟回合当前角色立即抢占该位置，然后直接进入下一回合的模拟。

3. 对于随机走子，可以尝试简单地更改权重，具体方法已在中庸之道一节描述。

这三条最简单的策略其实带来了性能上的隐忧。每一模拟回合中的两轮遍历比随机走子多了一次棋盘宽度的遍历（随机走子也要先遍历一次得到所有可落子点），并且每次遍历中由于要尝试某处落子后某方是否能立即胜利，还要花费更多的时间去判断是否获胜。可以预料到将会高度地影响到能够进行的模拟次数。

我实现了 `must` 方法后，发现对于对局结果影响差强人意，第二个方法实现了之后，反而胜率降低。第三个方法带来了稳定的提升。

由此可知，策略的存在牺牲了部分迭代次数，但提高了大量模拟中有效模拟（即贴近现实理性情况的模拟）次数的占比。因此，给定一定的时间资源，无法进行超大量模拟的前提下，把握好策略的复杂度，平衡迭代次数、模拟次数与有效率的 trade-off，是优化 MCTS 算法的关键。 

## 其他技巧

网站 https://www.xqbase.com/computer.htm 上介绍了位棋盘的数据结构，可以用于加速胜负的判断，从而进一步提高迭代次数。不过由于时间精力所限，本次实验中没有实现。

# 总结

最终胜率如下：

<img src="https://zhaochenyang20.github.io/pic/lecture/2022_spring/IAI/win.jpg" alt="MCTS" style="zoom:20%;" />

本次四子棋实验中，我学习并实现了 MCTS 算法，并在关于“随机模拟与策略”问题的实验和分析中加深了对该算法的理解，同时也练习了寻找问题并优化矛盾的思维方式。


# References


Guillaume Chaslot, Mark Winands, H. Herik, Jos Uiterwijk, and Bruno Bouzy. Progressive strategies for monte-carlo tree search. New Mathematics and Natural Computation, 04:343–357, 11 2008. 
